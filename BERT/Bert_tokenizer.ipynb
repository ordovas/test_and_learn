{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ordovas/test_and_learn/blob/main/Bert_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMRkIFhuTM9M"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "76HfPILdC5lD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y1h4YVFfDd1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.14.9)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.20.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.59.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-hub) (1.20.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-hub) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator==2.3.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-hub\n",
    "!pip install --upgrade tensorflow-estimator==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qMqTwu9jENrO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0_xu0I3jFP9"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FifCe97pTVql"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S0lOeu8TbnP"
   },
   "source": [
    "We import files from our personal Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aRCxQui8Gqi_"
   },
   "outputs": [],
   "source": [
    "#drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f6iT5nxDHLRz"
   },
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    \"training.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kKnCVewUIBkc"
   },
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xWWUo_XVeqoG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Quzx5tnjUtl"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8hlexmRjXIS"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jBSUDL-UP-W_"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Delete the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Delete URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Just keep letters and important punctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Remove additional spaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8jiMaQsLWiTS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\940700\\.conda\\envs\\datascience\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\940700\\.conda\\envs\\datascience\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EaqLE0fdWtni"
   },
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eh7sIquja5t"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV73IkgKUCmV"
   },
   "source": [
    "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0wry-st-HMN0"
   },
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LggMv7k7Z3Ij"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EGfTo5uIa2is"
   },
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-4oGSu5jxUi"
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaVPF9-rUTqZ"
   },
   "source": [
    "We will create padded batches (so we pad sentences for each batch inpedendently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HS_f6gWsLfLM"
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_inputs)]\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x: x[2])\n",
    "sorted_all = [(sent_lab[0], sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ry0uJJg8lSQR"
   },
   "outputs": [],
   "source": [
    "# A list is a type of iterator so it can be used as generator for a dataset\n",
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cF74g5hpYzaZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=int32, numpy=array([3666, 2204, 2214, 2188, 6876, 1048, 2863, 2080])>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZzHAhlfTlrcj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "s3ktdxCEm4Yh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n",
       " array([[ 3666,  2204,  2214,  2188,  6876,  1048,  2863,  2080],\n",
       "        [ 2008,  2015,  1037,  9202,  2518,  2000,  4299,   999],\n",
       "        [ 3403,  2006,  2000,  2131,  2067,  1012,  1012,  1012],\n",
       "        [ 1045,  3109,  3363,  4571, 10587,  3422,  2039,   999],\n",
       "        [ 2204,  2851,  3407, 15854,  2546, 15557,  5267, 10259],\n",
       "        [ 2748,  1037,  9129,  1997,  2149,  2302, 27263,  2015],\n",
       "        [ 2092,  2298,  2040,  2787,  2000,  2265,  2039,  1012],\n",
       "        [10166,  1012,  2732, 10313,  2001, 29350,  3819,  1012],\n",
       "        [ 1045,  3984,  2049,  2067,  2000, 22794,  2309,  2166],\n",
       "        [ 3087, 11281,  1037,  2208,  1997,  3329,  2666,  1029],\n",
       "        [ 2026,  2567,  2038,  2467,  2246,  2041,  2005,  2033],\n",
       "        [ 2012,  1996,  2208,  1012,  1012,  1012,  3030, 24057],\n",
       "        [ 2383,  1996,  9594,  2015,  5660,  2033,  2070,  4596],\n",
       "        [ 2204,  2851,  1056, 28394, 10814,  1012,  2572,  5305],\n",
       "        [ 2053,  9541,  2080,  1012,  2019,  7262,  3668,  3293],\n",
       "        [ 1045,  2228,  1045,  2031, 28246,  1012,  1012,  1012],\n",
       "        [ 4658,   999,   999,  1045,  2572,  2525,  2206,  2002],\n",
       "        [ 1045,  2113,  1012,  2027,  2031,  2009,  2182,  1012],\n",
       "        [12954,  2003,  2067,  2021,  2054,  2055,  9548,  1029],\n",
       "        [16021,  5358,  6200,  2025,  2204,  1012,  1012,  1012],\n",
       "        [ 1045,  2215,  2000,  2022,  2012,  1043,  8523,  3406],\n",
       "        [ 1044, 14227,  2213,  1012,  1012,  1012,  2204,  2391],\n",
       "        [ 2024,  2111,  2061,  4089,  2999,  1029,  2129,  6517],\n",
       "        [ 5236,  3902,  4303,  3053,  2701,  2006,  2033,  2153],\n",
       "        [ 3409,  2041,  2283, 16525,  6137,  5603, 23644,  4779],\n",
       "        [ 1045,  2064,  5047,  2505,  2008,  3310,  2026,  2126],\n",
       "        [ 2821,  2008,  2001,  5791, 24329,  1012,  1012,  1012],\n",
       "        [ 2619,  1999,  2142,  2163,  4669, 27263, 25725,  8795],\n",
       "        [ 1045,  2215,  2070, 11937,  3597,  4330,   999,   999],\n",
       "        [ 2085,  1012,  1045,  2097,  2707,  2026, 19453,  1012],\n",
       "        [12802,  1997,  2725,  2023,  1998,  2061,  2172,  2062],\n",
       "        [12842,  1045,  2064,  1005,  1056,  2156, 23205,  2080]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0])>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xrPqJeYpmfcv"
   },
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxONsFVHkFLU"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "L6DD3k3qPLDQ"
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1)\n",
    "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
    "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
    "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
    "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
    "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSix1l4jkIxp"
   },
   "source": [
    "# Stage 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lhfUFvWEPOIf"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VMtdiWmwv6rD"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6apbd7FrwPYo"
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "78cceSGCw1XC"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/MyDrive/projects/BERT/ckpt_bert_tok/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest Checkpoint restored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2YIF5trzx7RA"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrT8oWZzQNmW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  37196/Unknown - 3389s 91ms/step - loss: 0.4296 - accuracy: 0.8022Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 3390s 91ms/step - loss: 0.4296 - accuracy: 0.8022\n",
      "Epoch 2/5\n",
      " 9421/37196 [======>.......................] - ETA: 40:22 - loss: 0.3489 - accuracy: 0.8495"
     ]
    }
   ],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IiDW919kQQK"
   },
   "source": [
    "# Stage 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MthhNfnG1TPV"
   },
   "outputs": [],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-jrRvtl1xuk"
   },
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "    inputs = tf.expand_dims(tokens, 0)\n",
    "\n",
    "    output = Dcnn(inputs, training=False)\n",
    "\n",
    "    sentiment = math.floor(output*2)\n",
    "\n",
    "    if sentiment == 0:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: negative.\".format(\n",
    "            output))\n",
    "    elif sentiment == 1:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: positive.\".format(\n",
    "            output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kk8V2bdvwfCv"
   },
   "outputs": [],
   "source": [
    "get_prediction(\"This movie was pretty interesting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilgSppeGParJ"
   },
   "outputs": [],
   "source": [
    "get_prediction(\"I'd rather not do that again.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Bert_tokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
