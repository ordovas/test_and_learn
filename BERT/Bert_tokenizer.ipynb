{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ordovas/test_and_learn/blob/main/Bert_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1: Importing dependencies"
      ],
      "metadata": {
        "id": "RMRkIFhuTM9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "76HfPILdC5lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: bert-for-tf2 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.14.9)\nRequirement already satisfied: params-flow>=0.8.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from bert-for-tf2) (0.8.2)\nRequirement already satisfied: py-params>=0.9.6 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from bert-for-tf2) (0.10.2)\nRequirement already satisfied: numpy in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.20.2)\nRequirement already satisfied: tqdm in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.59.0)\nRequirement already satisfied: sentencepiece in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.1.96)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "y1h4YVFfDd1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-hub\n",
        "!pip install --upgrade tensorflow-estimator==2.3.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow-hub in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.12.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-hub) (1.20.2)\nRequirement already satisfied: protobuf>=3.8.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from tensorflow-hub) (3.14.0)\nRequirement already satisfied: six>=1.9 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\nRequirement already satisfied: tensorflow-estimator==2.3.0 in c:\\users\\940700\\.conda\\envs\\datascience\\lib\\site-packages (2.3.0)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "qMqTwu9jENrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Data preprocessing"
      ],
      "metadata": {
        "id": "f0_xu0I3jFP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading files"
      ],
      "metadata": {
        "id": "FifCe97pTVql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import files from our personal Google drive."
      ],
      "metadata": {
        "id": "6S0lOeu8TbnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drive.mount(\"/content/drive\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "aRCxQui8Gqi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"training.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "f6iT5nxDHLRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "kKnCVewUIBkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   sentiment                                               text\n0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1          0  is upset that he can't update his Facebook by ...\n2          0  @Kenichan I dived many times for the ball. Man...\n3          0    my whole body feels itchy and like its on fire \n4          0  @nationwideclass no, it's not behaving at all...."
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "xWWUo_XVeqoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "9Quzx5tnjUtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning"
      ],
      "metadata": {
        "id": "M8hlexmRjXIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Delete the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Delete URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Just keep letters and important punctuation\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Remove additional spaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "jBSUDL-UP-W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "C:\\Users\\940700\\.conda\\envs\\datascience\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n  warnings.warn(\nC:\\Users\\940700\\.conda\\envs\\datascience\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n  warnings.warn(\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "8jiMaQsLWiTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "EaqLE0fdWtni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "6eh7sIquja5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ],
      "metadata": {
        "id": "pV73IkgKUCmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "0wry-st-HMN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "LggMv7k7Z3Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "id": "EGfTo5uIa2is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset creation"
      ],
      "metadata": {
        "id": "B-4oGSu5jxUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create padded batches (so we pad sentences for each batch inpedendently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ],
      "metadata": {
        "id": "DaVPF9-rUTqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "HS_f6gWsLfLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list is a type of iterator so it can be used as generator for a dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "ry0uJJg8lSQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(all_dataset))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "(<tf.Tensor: shape=(8,), dtype=int32, numpy=array([3666, 2204, 2214, 2188, 6876, 1048, 2863, 2080])>,\n <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "id": "cF74g5hpYzaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "id": "ZzHAhlfTlrcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(all_batched))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n array([[ 3666,  2204,  2214,  2188,  6876,  1048,  2863,  2080],\n        [ 2008,  2015,  1037,  9202,  2518,  2000,  4299,   999],\n        [ 3403,  2006,  2000,  2131,  2067,  1012,  1012,  1012],\n        [ 1045,  3109,  3363,  4571, 10587,  3422,  2039,   999],\n        [ 2204,  2851,  3407, 15854,  2546, 15557,  5267, 10259],\n        [ 2748,  1037,  9129,  1997,  2149,  2302, 27263,  2015],\n        [ 2092,  2298,  2040,  2787,  2000,  2265,  2039,  1012],\n        [10166,  1012,  2732, 10313,  2001, 29350,  3819,  1012],\n        [ 1045,  3984,  2049,  2067,  2000, 22794,  2309,  2166],\n        [ 3087, 11281,  1037,  2208,  1997,  3329,  2666,  1029],\n        [ 2026,  2567,  2038,  2467,  2246,  2041,  2005,  2033],\n        [ 2012,  1996,  2208,  1012,  1012,  1012,  3030, 24057],\n        [ 2383,  1996,  9594,  2015,  5660,  2033,  2070,  4596],\n        [ 2204,  2851,  1056, 28394, 10814,  1012,  2572,  5305],\n        [ 2053,  9541,  2080,  1012,  2019,  7262,  3668,  3293],\n        [ 1045,  2228,  1045,  2031, 28246,  1012,  1012,  1012],\n        [ 4658,   999,   999,  1045,  2572,  2525,  2206,  2002],\n        [ 1045,  2113,  1012,  2027,  2031,  2009,  2182,  1012],\n        [12954,  2003,  2067,  2021,  2054,  2055,  9548,  1029],\n        [16021,  5358,  6200,  2025,  2204,  1012,  1012,  1012],\n        [ 1045,  2215,  2000,  2022,  2012,  1043,  8523,  3406],\n        [ 1044, 14227,  2213,  1012,  1012,  1012,  2204,  2391],\n        [ 2024,  2111,  2061,  4089,  2999,  1029,  2129,  6517],\n        [ 5236,  3902,  4303,  3053,  2701,  2006,  2033,  2153],\n        [ 3409,  2041,  2283, 16525,  6137,  5603, 23644,  4779],\n        [ 1045,  2064,  5047,  2505,  2008,  3310,  2026,  2126],\n        [ 2821,  2008,  2001,  5791, 24329,  1012,  1012,  1012],\n        [ 2619,  1999,  2142,  2163,  4669, 27263, 25725,  8795],\n        [ 1045,  2215,  2070, 11937,  3597,  4330,   999,   999],\n        [ 2085,  1012,  1045,  2097,  2707,  2026, 19453,  1012],\n        [12802,  1997,  2725,  2023,  1998,  2061,  2172,  2062],\n        [12842,  1045,  2064,  1005,  1056,  2156, 23205,  2080]])>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 0])>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "id": "s3ktdxCEm4Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "id": "xrPqJeYpmfcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3: Model building"
      ],
      "metadata": {
        "id": "VxONsFVHkFLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 4: Training"
      ],
      "metadata": {
        "id": "vSix1l4jkIxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "id": "lhfUFvWEPOIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "id": "VMtdiWmwv6rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "id": "6apbd7FrwPYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./drive/MyDrive/projects/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint restored!\")"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "id": "78cceSGCw1XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "id": "2YIF5trzx7RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n  37196/Unknown - 3389s 91ms/step - loss: 0.4296 - accuracy: 0.8022Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\n37196/37196 [==============================] - 3390s 91ms/step - loss: 0.4296 - accuracy: 0.8022\nEpoch 2/5\n 9421/37196 [======>.......................] - ETA: 40:22 - loss: 0.3489 - accuracy: 0.8495"
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "WrT8oWZzQNmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 5: Evaluation"
      ],
      "metadata": {
        "id": "3IiDW919kQQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "MthhNfnG1TPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: negative.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: positive.\".format(\n",
        "            output))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Z-jrRvtl1xuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kk8V2bdvwfCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"I'd rather not do that again.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ilgSppeGParJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Bert_tokenizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}