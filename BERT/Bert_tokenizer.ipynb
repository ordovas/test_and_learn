{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ordovas/test_and_learn/blob/main/Bert_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1: Importing dependencies"
      ],
      "metadata": {
        "id": "RMRkIFhuTM9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "#from google.colab import drive"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "76HfPILdC5lD",
        "gather": {
          "logged": 1632215803948
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert-for-tf2\n",
        "#!pip install sentencepiece"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "y1h4YVFfDd1t",
        "gather": {
          "logged": 1632215804207
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-hub\n",
        "!pip install --upgrade tensorflow-estimator==2.1.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already up-to-date: tensorflow-hub in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.12.0)\nRequirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-hub) (3.17.3)\nRequirement already satisfied, skipping upgrade: numpy>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-hub) (1.18.5)\nRequirement already satisfied, skipping upgrade: six>=1.9 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.16.0)\nRequirement already up-to-date: tensorflow-estimator==2.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (2.1.0)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1632213180991
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "qMqTwu9jENrO",
        "gather": {
          "logged": 1632215814602
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "import tensorflow_hub as hub"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632215814759
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from tensorflow.keras import layers\r\n",
        "import bert"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632215814924
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Data preprocessing"
      ],
      "metadata": {
        "id": "f0_xu0I3jFP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading files"
      ],
      "metadata": {
        "id": "FifCe97pTVql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import files from our personal Google drive."
      ],
      "metadata": {
        "id": "6S0lOeu8TbnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drive.mount(\"/content/drive\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "gather": {
          "logged": 1632215815124
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"training.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "f6iT5nxDHLRz",
        "gather": {
          "logged": 1632215830354
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "kKnCVewUIBkc",
        "gather": {
          "logged": 1632215830629
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "   sentiment                                               text\n0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1          0  is upset that he can't update his Facebook by ...\n2          0  @Kenichan I dived many times for the ball. Man...\n3          0    my whole body feels itchy and like its on fire \n4          0  @nationwideclass no, it's not behaving at all....",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "xWWUo_XVeqoG",
        "gather": {
          "logged": 1632215830931
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "9Quzx5tnjUtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning"
      ],
      "metadata": {
        "id": "M8hlexmRjXIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Delete the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Delete URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Just keep letters and important punctuation\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Remove additional spaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "jBSUDL-UP-W_",
        "gather": {
          "logged": 1632215831137
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: lxml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (4.6.3)\r\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.text)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "1600000"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632215905033
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = [ ]\r\n",
        "for i,tweet in enumerate(data.text):\r\n",
        "    data_clean.append(clean_tweet(tweet))\r\n",
        "    if i%5000==0:\r\n",
        "        print(i,\"out of 1600000 - \", round(100*(i/1600000),2),\"%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0 out of 1600000 -  0.0 %\n5000 out of 1600000 -  0.31 %\n10000 out of 1600000 -  0.62 %\n15000 out of 1600000 -  0.94 %\n20000 out of 1600000 -  1.25 %\n25000 out of 1600000 -  1.56 %\n30000 out of 1600000 -  1.88 %\n35000 out of 1600000 -  2.19 %\n40000 out of 1600000 -  2.5 %\n45000 out of 1600000 -  2.81 %\n50000 out of 1600000 -  3.12 %\n55000 out of 1600000 -  3.44 %\n60000 out of 1600000 -  3.75 %\n65000 out of 1600000 -  4.06 %\n70000 out of 1600000 -  4.38 %\n75000 out of 1600000 -  4.69 %\n80000 out of 1600000 -  5.0 %\n85000 out of 1600000 -  5.31 %\n90000 out of 1600000 -  5.62 %\n95000 out of 1600000 -  5.94 %\n100000 out of 1600000 -  6.25 %\n105000 out of 1600000 -  6.56 %\n110000 out of 1600000 -  6.88 %\n115000 out of 1600000 -  7.19 %\n120000 out of 1600000 -  7.5 %\n125000 out of 1600000 -  7.81 %\n130000 out of 1600000 -  8.12 %\n135000 out of 1600000 -  8.44 %\n140000 out of 1600000 -  8.75 %\n145000 out of 1600000 -  9.06 %\n150000 out of 1600000 -  9.38 %\n155000 out of 1600000 -  9.69 %\n160000 out of 1600000 -  10.0 %\n165000 out of 1600000 -  10.31 %\n170000 out of 1600000 -  10.62 %\n175000 out of 1600000 -  10.94 %\n180000 out of 1600000 -  11.25 %\n185000 out of 1600000 -  11.56 %\n190000 out of 1600000 -  11.88 %\n195000 out of 1600000 -  12.19 %\n200000 out of 1600000 -  12.5 %\n205000 out of 1600000 -  12.81 %\n210000 out of 1600000 -  13.12 %\n215000 out of 1600000 -  13.44 %\n220000 out of 1600000 -  13.75 %\n225000 out of 1600000 -  14.06 %\n230000 out of 1600000 -  14.37 %\n235000 out of 1600000 -  14.69 %\n240000 out of 1600000 -  15.0 %\n245000 out of 1600000 -  15.31 %\n250000 out of 1600000 -  15.62 %\n255000 out of 1600000 -  15.94 %\n260000 out of 1600000 -  16.25 %\n265000 out of 1600000 -  16.56 %\n270000 out of 1600000 -  16.88 %\n275000 out of 1600000 -  17.19 %\n280000 out of 1600000 -  17.5 %\n285000 out of 1600000 -  17.81 %\n290000 out of 1600000 -  18.12 %\n295000 out of 1600000 -  18.44 %\n300000 out of 1600000 -  18.75 %\n305000 out of 1600000 -  19.06 %\n310000 out of 1600000 -  19.38 %\n315000 out of 1600000 -  19.69 %\n320000 out of 1600000 -  20.0 %\n325000 out of 1600000 -  20.31 %\n330000 out of 1600000 -  20.62 %\n335000 out of 1600000 -  20.94 %\n340000 out of 1600000 -  21.25 %\n345000 out of 1600000 -  21.56 %\n350000 out of 1600000 -  21.88 %\n355000 out of 1600000 -  22.19 %\n360000 out of 1600000 -  22.5 %\n365000 out of 1600000 -  22.81 %\n370000 out of 1600000 -  23.12 %\n375000 out of 1600000 -  23.44 %\n380000 out of 1600000 -  23.75 %\n385000 out of 1600000 -  24.06 %\n390000 out of 1600000 -  24.38 %\n395000 out of 1600000 -  24.69 %\n400000 out of 1600000 -  25.0 %\n405000 out of 1600000 -  25.31 %\n410000 out of 1600000 -  25.62 %\n415000 out of 1600000 -  25.94 %\n420000 out of 1600000 -  26.25 %\n425000 out of 1600000 -  26.56 %\n430000 out of 1600000 -  26.88 %\n435000 out of 1600000 -  27.19 %\n440000 out of 1600000 -  27.5 %\n445000 out of 1600000 -  27.81 %\n450000 out of 1600000 -  28.12 %\n455000 out of 1600000 -  28.44 %\n460000 out of 1600000 -  28.75 %\n465000 out of 1600000 -  29.06 %\n470000 out of 1600000 -  29.38 %\n475000 out of 1600000 -  29.69 %\n480000 out of 1600000 -  30.0 %\n485000 out of 1600000 -  30.31 %\n490000 out of 1600000 -  30.63 %\n495000 out of 1600000 -  30.94 %\n500000 out of 1600000 -  31.25 %\n505000 out of 1600000 -  31.56 %\n510000 out of 1600000 -  31.87 %\n515000 out of 1600000 -  32.19 %\n520000 out of 1600000 -  32.5 %\n525000 out of 1600000 -  32.81 %\n530000 out of 1600000 -  33.12 %\n535000 out of 1600000 -  33.44 %\n540000 out of 1600000 -  33.75 %\n545000 out of 1600000 -  34.06 %\n550000 out of 1600000 -  34.38 %\n555000 out of 1600000 -  34.69 %\n560000 out of 1600000 -  35.0 %\n565000 out of 1600000 -  35.31 %\n570000 out of 1600000 -  35.62 %\n575000 out of 1600000 -  35.94 %\n580000 out of 1600000 -  36.25 %\n585000 out of 1600000 -  36.56 %\n590000 out of 1600000 -  36.88 %\n595000 out of 1600000 -  37.19 %\n600000 out of 1600000 -  37.5 %\n605000 out of 1600000 -  37.81 %\n610000 out of 1600000 -  38.12 %\n615000 out of 1600000 -  38.44 %\n620000 out of 1600000 -  38.75 %\n625000 out of 1600000 -  39.06 %\n630000 out of 1600000 -  39.38 %\n635000 out of 1600000 -  39.69 %\n640000 out of 1600000 -  40.0 %\n645000 out of 1600000 -  40.31 %\n650000 out of 1600000 -  40.62 %\n655000 out of 1600000 -  40.94 %\n660000 out of 1600000 -  41.25 %\n665000 out of 1600000 -  41.56 %\n670000 out of 1600000 -  41.88 %\n675000 out of 1600000 -  42.19 %\n680000 out of 1600000 -  42.5 %\n685000 out of 1600000 -  42.81 %\n690000 out of 1600000 -  43.12 %\n695000 out of 1600000 -  43.44 %\n700000 out of 1600000 -  43.75 %\n705000 out of 1600000 -  44.06 %\n710000 out of 1600000 -  44.38 %\n715000 out of 1600000 -  44.69 %\n720000 out of 1600000 -  45.0 %\n725000 out of 1600000 -  45.31 %\n730000 out of 1600000 -  45.62 %\n735000 out of 1600000 -  45.94 %\n740000 out of 1600000 -  46.25 %\n745000 out of 1600000 -  46.56 %\n750000 out of 1600000 -  46.88 %\n755000 out of 1600000 -  47.19 %\n760000 out of 1600000 -  47.5 %\n765000 out of 1600000 -  47.81 %\n770000 out of 1600000 -  48.12 %\n775000 out of 1600000 -  48.44 %\n780000 out of 1600000 -  48.75 %\n785000 out of 1600000 -  49.06 %\n790000 out of 1600000 -  49.38 %\n795000 out of 1600000 -  49.69 %\n800000 out of 1600000 -  50.0 %\n805000 out of 1600000 -  50.31 %\n810000 out of 1600000 -  50.62 %\n815000 out of 1600000 -  50.94 %\n820000 out of 1600000 -  51.25 %\n825000 out of 1600000 -  51.56 %\n830000 out of 1600000 -  51.88 %\n835000 out of 1600000 -  52.19 %\n840000 out of 1600000 -  52.5 %\n845000 out of 1600000 -  52.81 %\n850000 out of 1600000 -  53.12 %\n855000 out of 1600000 -  53.44 %\n860000 out of 1600000 -  53.75 %\n865000 out of 1600000 -  54.06 %\n870000 out of 1600000 -  54.37 %\n875000 out of 1600000 -  54.69 %\n880000 out of 1600000 -  55.0 %\n885000 out of 1600000 -  55.31 %\n890000 out of 1600000 -  55.62 %\n895000 out of 1600000 -  55.94 %\n900000 out of 1600000 -  56.25 %\n905000 out of 1600000 -  56.56 %\n910000 out of 1600000 -  56.88 %\n915000 out of 1600000 -  57.19 %\n920000 out of 1600000 -  57.5 %\n925000 out of 1600000 -  57.81 %\n930000 out of 1600000 -  58.13 %\n935000 out of 1600000 -  58.44 %\n940000 out of 1600000 -  58.75 %\n945000 out of 1600000 -  59.06 %\n950000 out of 1600000 -  59.38 %\n955000 out of 1600000 -  59.69 %\n960000 out of 1600000 -  60.0 %\n965000 out of 1600000 -  60.31 %\n970000 out of 1600000 -  60.62 %\n975000 out of 1600000 -  60.94 %\n980000 out of 1600000 -  61.25 %\n985000 out of 1600000 -  61.56 %\n990000 out of 1600000 -  61.88 %\n995000 out of 1600000 -  62.19 %\n1000000 out of 1600000 -  62.5 %\n1005000 out of 1600000 -  62.81 %\n1010000 out of 1600000 -  63.12 %\n1015000 out of 1600000 -  63.44 %\n1020000 out of 1600000 -  63.75 %\n1025000 out of 1600000 -  64.06 %\n1030000 out of 1600000 -  64.38 %\n1035000 out of 1600000 -  64.69 %\n1040000 out of 1600000 -  65.0 %\n1045000 out of 1600000 -  65.31 %\n1050000 out of 1600000 -  65.62 %\n1055000 out of 1600000 -  65.94 %\n1060000 out of 1600000 -  66.25 %\n1065000 out of 1600000 -  66.56 %\n1070000 out of 1600000 -  66.88 %\n1075000 out of 1600000 -  67.19 %\n1080000 out of 1600000 -  67.5 %\n1085000 out of 1600000 -  67.81 %\n1090000 out of 1600000 -  68.12 %\n1095000 out of 1600000 -  68.44 %\n1100000 out of 1600000 -  68.75 %\n1105000 out of 1600000 -  69.06 %\n1110000 out of 1600000 -  69.38 %\n1115000 out of 1600000 -  69.69 %\n1120000 out of 1600000 -  70.0 %\n1125000 out of 1600000 -  70.31 %\n1130000 out of 1600000 -  70.62 %\n1135000 out of 1600000 -  70.94 %\n1140000 out of 1600000 -  71.25 %\n1145000 out of 1600000 -  71.56 %\n1150000 out of 1600000 -  71.88 %\n1155000 out of 1600000 -  72.19 %\n1160000 out of 1600000 -  72.5 %\n1165000 out of 1600000 -  72.81 %\n1170000 out of 1600000 -  73.12 %\n1175000 out of 1600000 -  73.44 %\n1180000 out of 1600000 -  73.75 %\n1185000 out of 1600000 -  74.06 %\n1190000 out of 1600000 -  74.38 %\n1195000 out of 1600000 -  74.69 %\n1200000 out of 1600000 -  75.0 %\n1205000 out of 1600000 -  75.31 %\n1210000 out of 1600000 -  75.62 %\n1215000 out of 1600000 -  75.94 %\n1220000 out of 1600000 -  76.25 %\n1225000 out of 1600000 -  76.56 %\n1230000 out of 1600000 -  76.88 %\n1235000 out of 1600000 -  77.19 %\n1240000 out of 1600000 -  77.5 %\n1245000 out of 1600000 -  77.81 %\n1250000 out of 1600000 -  78.12 %\n1255000 out of 1600000 -  78.44 %\n1260000 out of 1600000 -  78.75 %\n1265000 out of 1600000 -  79.06 %\n1270000 out of 1600000 -  79.38 %\n1275000 out of 1600000 -  79.69 %\n1280000 out of 1600000 -  80.0 %\n1285000 out of 1600000 -  80.31 %\n1290000 out of 1600000 -  80.62 %\n1295000 out of 1600000 -  80.94 %\n1300000 out of 1600000 -  81.25 %\n1305000 out of 1600000 -  81.56 %\n1310000 out of 1600000 -  81.88 %\n1315000 out of 1600000 -  82.19 %\n1320000 out of 1600000 -  82.5 %\n1325000 out of 1600000 -  82.81 %\n1330000 out of 1600000 -  83.12 %\n1335000 out of 1600000 -  83.44 %\n1340000 out of 1600000 -  83.75 %\n1345000 out of 1600000 -  84.06 %\n1350000 out of 1600000 -  84.38 %\n1355000 out of 1600000 -  84.69 %\n1360000 out of 1600000 -  85.0 %\n1365000 out of 1600000 -  85.31 %\n1370000 out of 1600000 -  85.62 %\n1375000 out of 1600000 -  85.94 %\n1380000 out of 1600000 -  86.25 %\n1385000 out of 1600000 -  86.56 %\n1390000 out of 1600000 -  86.88 %\n1395000 out of 1600000 -  87.19 %\n1400000 out of 1600000 -  87.5 %\n1405000 out of 1600000 -  87.81 %\n1410000 out of 1600000 -  88.12 %\n1415000 out of 1600000 -  88.44 %\n1420000 out of 1600000 -  88.75 %\n1425000 out of 1600000 -  89.06 %\n1430000 out of 1600000 -  89.38 %\n1435000 out of 1600000 -  89.69 %\n1440000 out of 1600000 -  90.0 %\n1445000 out of 1600000 -  90.31 %\n1450000 out of 1600000 -  90.62 %\n1455000 out of 1600000 -  90.94 %\n1460000 out of 1600000 -  91.25 %\n1465000 out of 1600000 -  91.56 %\n1470000 out of 1600000 -  91.88 %\n1475000 out of 1600000 -  92.19 %\n1480000 out of 1600000 -  92.5 %\n1485000 out of 1600000 -  92.81 %\n1490000 out of 1600000 -  93.12 %\n1495000 out of 1600000 -  93.44 %\n1500000 out of 1600000 -  93.75 %\n1505000 out of 1600000 -  94.06 %\n1510000 out of 1600000 -  94.38 %\n1515000 out of 1600000 -  94.69 %\n1520000 out of 1600000 -  95.0 %\n1525000 out of 1600000 -  95.31 %\n1530000 out of 1600000 -  95.62 %\n1535000 out of 1600000 -  95.94 %\n1540000 out of 1600000 -  96.25 %\n1545000 out of 1600000 -  96.56 %\n1550000 out of 1600000 -  96.88 %\n1555000 out of 1600000 -  97.19 %\n1560000 out of 1600000 -  97.5 %\n1565000 out of 1600000 -  97.81 %\n1570000 out of 1600000 -  98.12 %\n1575000 out of 1600000 -  98.44 %\n1580000 out of 1600000 -  98.75 %\n1585000 out of 1600000 -  99.06 %\n1590000 out of 1600000 -  99.38 %\n1595000 out of 1600000 -  99.69 %\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632221180079
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8jiMaQsLWiTS",
        "gather": {
          "logged": 1632213108469
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "id": "EaqLE0fdWtni",
        "gather": {
          "logged": 1632221180609
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "6eh7sIquja5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ],
      "metadata": {
        "id": "pV73IkgKUCmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "id": "0wry-st-HMN0",
        "gather": {
          "logged": 1632221193107
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "id": "LggMv7k7Z3Ij",
        "gather": {
          "logged": 1632221193377
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "id": "EGfTo5uIa2is",
        "gather": {
          "logged": 1632221572564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset creation"
      ],
      "metadata": {
        "id": "B-4oGSu5jxUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create padded batches (so we pad sentences for each batch inpedendently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ],
      "metadata": {
        "id": "DaVPF9-rUTqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]  #in vid 18 it uses >2"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "id": "HS_f6gWsLfLM",
        "gather": {
          "logged": 1632221580642
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list is a type of iterator so it can be used as generator for a dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "id": "ry0uJJg8lSQR",
        "gather": {
          "logged": 1632221580808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(all_dataset))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": "(<tf.Tensor: shape=(8,), dtype=int32, numpy=\n array([ 1045,  4299,  2009,  2347,  1005,  1056, 16373,  1012],\n       dtype=int32)>,\n <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {
        "id": "cF74g5hpYzaZ",
        "gather": {
          "logged": 1632234876910
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()) )"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "id": "ZzHAhlfTlrcj",
        "gather": {
          "logged": 1632221581338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(all_batched))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n array([[ 1045,  4299,  2009,  2347,  1005,  1056, 16373,  1012],\n        [10364,  6861,  2035,  2058,  2010,  9019,  1012, 26316],\n        [ 2040,  1029,  2129,  1029,  2009,  2035,  2047,   999],\n        [ 2205,  2919,  2057,  2031,  2000,  3524,  2019,  3178],\n        [ 6842,  2003, 15729,  2033,  2039,  2023,  2851,  1012],\n        [ 1045,  1005,  1049,  2182,  2049,  2980,  2004,  3109],\n        [ 1045,  4060,  2026,  4451,  2205,  2172,  1012,  1012],\n        [ 3398,  2074,  2318,  1996,  8065,  2023,  2095,  1012],\n        [10506,  5292,  3270,  3270,  3270,  1045,  2097,  2085],\n        [ 1048,  3398,  2034,  2048,  2086,  2053, 23605,  2395],\n        [ 2293,  2035,  1996,  8403,  2111,  2012,  2147,   999],\n        [ 2178,  5353,  2985,  2012,  2188,  2894,  4718,  2140],\n        [12392,  2906,  1045,  2572,  7653,  1012,  4067,  2017],\n        [ 1045,  2018,  1996,  5409,  3637,  2412,   999,   999],\n        [ 1045,  2633,  2288,  9436,  1997,  1996,  2630,  7925],\n        [ 2003, 23042,  2000,  8738,  2852, 11565,  1012, 22708],\n        [ 4931,  4907,   999,  2129,  2024,  8038,  1005,  1029],\n        [ 2035,  2023,  4174,  2003,  2437,  1996,  4412,  5390],\n        [ 1045,  2293,  1996,  2081,  1999,  3915,  8840,  2140],\n        [ 2012,  2147,  1012,  1012,  1012, 14436,  2075,  9317],\n        [ 1045,  4299,  1045,  2001,  1999,  8423,  2157,  2085],\n        [ 2026, 27048,  2134,  2102,  3500,  2067,  2039,   999],\n        [ 7929,  2061,  1045,  2572,  2062,  9081,  2084, 27261],\n        [ 2288,  1998,  8288,  2006,  2026,  2132,  1012, 13403],\n        [22822,  3654,  8747,  7315,  2054,  1037,  6517,  2518],\n        [ 2034,  2154,  2067,  2012, 15315, 13669,  2001,  4569],\n        [13131,  3784,  4062,  1005,  1055,  3968,  2607,  1012],\n        [ 2339,  3475,  1005,  1056,  1996,  3103,  2041,  1029],\n        [ 2009,  1005,  1055, 14533,  2051, 15854,  7570,  2080],\n        [ 4931, 24471,  2025,  2206,  2033,   999,  1029,   999],\n        [ 5292,  3270,  3374,  2055,  2008,  2074, 21603,  4569],\n        [ 2748,  1045,  2079,  2175, 16371, 13871,  8454,   999]],\n       dtype=int32)>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n        1, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int32)>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "id": "s3ktdxCEm4Yh",
        "gather": {
          "logged": 1632221581522
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "id": "xrPqJeYpmfcv",
        "gather": {
          "logged": 1632221581686
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_BATCHES, NB_BATCHES_TEST"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 58,
          "data": {
            "text/plain": "(41328, 4132)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 58,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632235095360
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'PaddedBatchDataset' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-5ef609ed4795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'PaddedBatchDataset' object is not subscriptable"
          ]
        }
      ],
      "execution_count": 63,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3: Model building"
      ],
      "metadata": {
        "id": "VxONsFVHkFLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "id": "L6DD3k3qPLDQ",
        "gather": {
          "logged": 1632221582006
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 4: Training"
      ],
      "metadata": {
        "id": "vSix1l4jkIxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "id": "lhfUFvWEPOIf",
        "gather": {
          "logged": 1632221582240
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "id": "VMtdiWmwv6rD",
        "gather": {
          "logged": 1632221582407
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "id": "6apbd7FrwPYo",
        "gather": {
          "logged": 1632221582589
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./drive/MyDrive/projects/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint restored!\")"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "id": "78cceSGCw1XC",
        "gather": {
          "logged": 1632221582860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "id": "2YIF5trzx7RA",
        "gather": {
          "logged": 1632221583008
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n  37196/Unknown - 2172s 58ms/step - loss: 0.4297 - accuracy: 0.8021Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\b\b\n37196/37196 [==============================] - 2174s 58ms/step - loss: 0.4297 - accuracy: 0.8021\nEpoch 2/5\n37195/37196 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8296Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n37196/37196 [==============================] - 2047s 55ms/step - loss: 0.3825 - accuracy: 0.8296\nEpoch 3/5\n37195/37196 [============================>.] - ETA: 0s - loss: 0.3435 - accuracy: 0.8503Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n37196/37196 [==============================] - 2079s 56ms/step - loss: 0.3435 - accuracy: 0.8503\nEpoch 4/5\n37195/37196 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8696Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n37196/37196 [==============================] - 2111s 57ms/step - loss: 0.3036 - accuracy: 0.8696\nEpoch 5/5\n37195/37196 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8871Checkpoint saved at ./drive/MyDrive/projects/BERT/ckpt_bert_tok/.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n37196/37196 [==============================] - 2040s 55ms/step - loss: 0.2657 - accuracy: 0.8871\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0e4a8044a8>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "gather": {
          "logged": 1632232052491
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 5: Evaluation"
      ],
      "metadata": {
        "id": "3IiDW919kQQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   4132/Unknown - 37s 9ms/step - loss: 0.4357 - accuracy: 0.8340[0.4356998347344611, 0.83400136]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "id": "MthhNfnG1TPV",
        "gather": {
          "logged": 1632232090043
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "    print(inputs)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "    print(output)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: negative.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: positive.\".format(\n",
        "            output))"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "id": "Z-jrRvtl1xuk",
        "gather": {
          "logged": 1632232745774
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[2023 3185 2001 3492 5875 1012]], shape=(1, 6), dtype=int32)\ntf.Tensor([[0.9992982]], shape=(1, 1), dtype=float32)\nOutput of the model: [[0.9992982]]\nPredicted sentiment: positive.\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "id": "kk8V2bdvwfCv",
        "gather": {
          "logged": 1632232746832
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"I'd rather not do that again.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[1045 1005 1040 2738 2025 2079 2008 2153 1012]], shape=(1, 9), dtype=int32)\ntf.Tensor([[0.1349157]], shape=(1, 1), dtype=float32)\nOutput of the model: [[0.1349157]]\nPredicted sentiment: negative.\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "id": "ilgSppeGParJ",
        "gather": {
          "logged": 1632232916043
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample=\"\"\"\r\n",
        "How many roads must a man walk down\r\n",
        "Before you call him a man?\r\n",
        "How many seas must a white dove sail\r\n",
        "Before she sleeps in the sand?\r\n",
        "Yes, and how many times must the cannonballs fly\r\n",
        "Before they're forever banned?\r\n",
        "The answer, my friend, is blowin' in the wind\r\n",
        "The answer is blowin' in the wind\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "get_prediction(text_sample)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor(\n[[ 2129  2116  4925  2442  1037  2158  3328  2091  2077  2017  2655  2032\n   1037  2158  1029  2129  2116 11915  2442  1037  2317 10855  9498  2077\n   2016 25126  1999  1996  5472  1029  2748  1010  1998  2129  2116  2335\n   2442  1996  8854 18510  4875  2077  2027  1005  2128  5091  7917  1029\n   1996  3437  1010  2026  2767  1010  2003  6271  2378  1005  1999  1996\n   3612  1996  3437  2003  6271  2378  1005  1999  1996  3612]], shape=(1, 70), dtype=int32)\ntf.Tensor([[0.10389104]], shape=(1, 1), dtype=float32)\nOutput of the model: [[0.10389104]]\nPredicted sentiment: negative.\n"
        }
      ],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632233111692
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample=\"\"\"\r\n",
        "Here comes the sun do, do, do\r\n",
        "Here comes the sun\r\n",
        "And I say it's all right\r\n",
        "Little darling, it's been a long cold lonely winter\r\n",
        "Little darling, it seems like years since it's been here\r\n",
        "Here comes the sun do, do, do\r\n",
        "Here comes the sun\r\n",
        "And I say it's all right\r\n",
        "Little darling, the smiles returning to the faces\r\n",
        "Little darling, it feels like years since it's been here\r\n",
        "Here comes the sun do, do, do\r\n",
        "Here comes the sun\r\n",
        "And I say it's all right\r\n",
        "Little darling, I feel that ice is slowly melting\r\n",
        "Little darling, it seems like years since it's been clear\r\n",
        "Here comes the sun do, do, do\r\n",
        "Here comes the sun\r\n",
        "And I say it's all right\r\n",
        "Here comes the sun do, do, do\r\n",
        "Here comes the sun\r\n",
        "And I say it's all right\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "get_prediction(text_sample)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor(\n[[ 2182  3310  1996  3103  2079  1010  2079  1010  2079  2182  3310  1996\n   3103  1998  1045  2360  2009  1005  1055  2035  2157  2210  9548  1010\n   2009  1005  1055  2042  1037  2146  3147  9479  3467  2210  9548  1010\n   2009  3849  2066  2086  2144  2009  1005  1055  2042  2182  2182  3310\n   1996  3103  2079  1010  2079  1010  2079  2182  3310  1996  3103  1998\n   1045  2360  2009  1005  1055  2035  2157  2210  9548  1010  1996  8451\n   4192  2000  1996  5344  2210  9548  1010  2009  5683  2066  2086  2144\n   2009  1005  1055  2042  2182  2182  3310  1996  3103  2079  1010  2079\n   1010  2079  2182  3310  1996  3103  1998  1045  2360  2009  1005  1055\n   2035  2157  2210  9548  1010  1045  2514  2008  3256  2003  3254 13721\n   2210  9548  1010  2009  3849  2066  2086  2144  2009  1005  1055  2042\n   3154  2182  3310  1996  3103  2079  1010  2079  1010  2079  2182  3310\n   1996  3103  1998  1045  2360  2009  1005  1055  2035  2157  2182  3310\n   1996  3103  2079  1010  2079  1010  2079  2182  3310  1996  3103  1998\n   1045  2360  2009  1005  1055  2035  2157]], shape=(1, 175), dtype=int32)\ntf.Tensor([[0.7619715]], shape=(1, 1), dtype=float32)\nOutput of the model: [[0.7619715]]\nPredicted sentiment: positive.\n"
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632233155731
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632239879352
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632239978344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Bert_tokenizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}